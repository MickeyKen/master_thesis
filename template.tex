%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 卒論，修論用のテンプレート
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass[12pt]{sonota/aislab}

% colorパッケージをgraphicxの前に読み込まないとトラブルになるようである．
% listingsの中でcolorを使用
\usepackage[usenames]{color}
% 図の挿入にgraphicxパッケージを使用．
% dvipdfmxを使用してpdfを作成することを前提とする．
\usepackage[dvipdfmx]{graphicx}
% urlの表記
\usepackage{url}
% \screen などのコマンドを使えるようにする．
\usepackage{ascmac}
% 書体の変更を許すtt環境
\usepackage{alltt}
% \begin{comment} 〜 \end{comment}でコメントアウトするためのコマンド
\usepackage{comment}

\usepackage{url}

\usepackage{listings}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 李研テンプレートを作成するにあたって追加したパッケージ
% 参考文献を参照する際の\citeの挙動
\usepackage{sonota/styles/cite}
% 図を横に並べる際に使用
\usepackage{sonota/styles/subfigure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% 図はfigsのサブディレクトリに集めることにする．
\graphicspath{{./figs/}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% ページのレイアウトの指定にgeometryパッケージを使用する
%
% jsbookは10ptでタイプセットし，それを12ptに拡大しているので，ここでの
% papersizeの指定は要注意（a4paperを指定すると結果的に縮小される）
% また，jsbookを使っているためtruemmを使う必要あり
\usepackage{geometry}
% 一行 40文字
\geometry{textwidth=40zw}
% テキスト領域を左右中央に
\geometry{hcentering=true}
% １ページ35行
\geometry{lines=35}
% footnoteとの間を１行分あける
\geometry{footskip=2\baselineskip}
% PDFへの出力を前提として，綴じ代はプリンタドライバの機能に任せる方針で
% 綴じ代はなしにする
\geometry{bindingoffset=0truemm}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% ヘッダ，フッタにfancyhdrを使う
%
\usepackage{fancyhdr}
\fancyhf{}
\fancyfoot[c]{\thepage}
\renewcommand{\headrulewidth}{0pt}
\renewcommand{\footrulewidth}{0pt}
\pagestyle{fancy}
% jsbook.clsでは，chapterの最初のページのpagestyleを
% plainではなく，plainhead ないし plainfootに設定している．
% そこで，plainheadを再定義する．
\fancypagestyle{plainhead}{%
%\fancypagestyle{plain}{%
\fancyhf{}
\fancyfoot[c]{\thepage}
\renewcommand{\headrulewidth}{0pt}
\renewcommand{\footrulewidth}{0pt}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% PDF版でしおりを挿入するためのスタイル hyperref
% dvipdfmxを使ってpdfファイルを作成することを前提とする
\usepackage{atbegshi}
\AtBeginShipoutFirst{\special{pdf:tounicode 90ms-RKSJ-UCS2}}
% MacTeX-2011では90ms-RKSJ-UCS2が含まれていない．
% 代わりにEUC-UCS2を指定してもうまく動くようである
%\AtBeginShipoutFirst{\special{pdf:tounicode EUC-UCS2}}
% paper sizeを変えないようにsetpagesize=falseを指定
% しおりに節番号などをつけるようにする．
\usepackage[setpagesize=false,dvipdfm,%
bookmarks=true,bookmarksnumbered=true]{hyperref}
%bookmarks=true,bookmarksnumbered=true,colorlinks=true]{hyperref}

\newcommand{\fig}[1]{
	Fig. #1
}
\newcommand{\tab}[1]{
	Table #1
}
\newcommand{\eq}[1]{
	式(#1)
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 卒業論文用テンプレート
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% △ここまでプリアンブル△
% △ここまでプリアンブル△
% △ここまでプリアンブル△
\begin{document}
% ▽ここから本文▽
% ▽ここから本文▽
% ▽ここから本文▽

\pdfbookmark[0]{表紙}{cover}
\begin{titlepage}
\centering
\vspace*{3truecm}
\Huge
% ここに卒論の題目を入れる
%Dual ubiquitous Displayによる\\
%深層強化学習を用いた\\
%行動モデルの構築\\

%情報投影ロボットの効率的な情報支援を目的とした\\
%Dual Ubiquitous Displayの開発と\\
%深層強化学習を用いた行動制御に関する研究\\

二光源情報投影ロボットの提案と\\
深層強化学習を用いた\\
行動制御に関する研究
\vfill
\Large
立命館大学\\
情報理工学研究科\\
博士課程前期課程 情報理工学専攻\\[1zh]
学生証番号 6611190063-0\\[0.5zh]
{\LARGE{三木健汰}}\\[2zh]
2021年2月
\end{titlepage}


\let\cleardoublepage\clearpage

% ページ番号はローマ数字で
\frontmatter

\clearpage
\pdfbookmark[0]{概要}{abstract}
\chapter*{概要}
本研究では二光源情報投影ロボットの開発と深層強化学習を用いた情報投影ロボットの行動制御を提案した．本研究室では情報投影ロボットUbiquitous Displayの実用化を目指して，公共の場で人と共存するための行動モデルの開発が行われてきた．しかし，それらの行動モデルはモデルベースな手法で構築されていることから，計算量が膨大となり必ずしも効率的な行動選択が出来ない問題点があった．そこで，行動モデルの計算量の軽量化と効率的な情報支援を目的として，ハードウェアの点ではプロジェクタを2台搭載した二光源情報投影ロボットの提案とソフトウェアの点では情報投影するための最適な行動選択を深層強化学習の技術の1つであるDQN(Deep Q-Network)により学習する手法を提案した．実験では，提案モデルの有用性を検証するため，シミュレーション環境で行動を学習した後，実世界で動作実験を行った．実験結果から壁などの障害物に衝突することなく情報投影するための行動獲得が可能であることを示した．

%% 目次
\clearpage
\pdfbookmark[0]{\contentsname}{toc}
\bgroup

% 章の目次
\tableofcontents
% 図の目次
\clearpage
\pdfbookmark[0]{\listfigurename}{lof}
\listoffigures 
% 表の目次 -- 表がない場合は，表目次そのもの削除すること．
\clearpage
\pdfbookmark[0]{\listtablename}{lot}
\listoftables

\egroup

% 本文スタート（ページ番号は算用数字にする）
\mainmatter

\chapter{緒言}\label{chap:introduction}

\section{研究背景}
近年のコンピュータ技術の進歩に伴いスマートフォンなどの小型情報端末が普及してきており，人々はどこでも様々な情報を取得できるようになってきている．そして，FacebookなどのSNS(Social Networking Service)により，簡単に個人の情報をデジタル化し発信することがでできると共に，情報量は膨大となり，情報の種類は多様化している．そのような情報化社会では，人は適切なときに自らが本当に必要な情報を取得するとこは困難になりつつある．こういった問題から，人が情報を探すのではなく，情報から人へ向けた能動的な情報支援に変化してきている．
GPSやbeaconなどによる位置情報を利用し，場所に応じてそのユーザに適した情報提供が可能となっている．しかし，移動時のスマートフォンの利用（歩きスマホ）は，周辺の人物や障害物への衝突や階段からの転落などの様々な問題があり，その危険性が報告されている\cite{aruki}．また，スマートフォンなどを持っていないユーザへは情報提供が行えない問題点もある．一方，環境設置型の情報提供手段として，駅構内や大型公共施設に設置された自動電子販売機や電子看板などにおいて，カメラを用いて利用者の性別や年齢などの特徴を取得し，その利用者の好みに応じた商品を推薦するシステムもある．しかし，こういった環境接地型の情報提供では，ユーザがその場所まで行く必要があるといった問題がある．
\\ \indent
これに対し，ユーザがスマートフォンなどの端末を持つ必要がなく，また情報提供場所まで行く必要がない情報提示を目指し，移動ロボットを用いた情報提示が行われている\cite{spencr}\cite{UD}．これらのロボットは，荷物の運搬などの単調な作業を行うだけでなく，空港やショッピングモールといった公共空間において，タッチディスプレイやプロジェクタを用いて，人に視覚的な情報の支援を目的として開発されてきたコミュニケーションロボットの一例である．
\section{先行研究}
以上のニーズを踏まえ,立命館大学李研究室では人に対して効果的に情報を提供しコミュニケーションを行うロボットとして，Ubiquitous Display(以後 UD と呼ぶ)の研究・開発を行ってきた\cite{UD1}\cite{UD2}\cite{UD3}\cite{UD4}\cite{UD5}.UD は移動プラットフォームに,パン・チルト回転機構とプロジェクタを搭載した移動型情報投影ロボットである．UD 自らが移動しプロジェクタにより情報を提供することで視覚的情報支援を行うことができる(Fig.\ref{projection}).この投影した視覚情報を用いたインタラクションにより人とコミュニケーションを行うことで，ロボットを用いた新たな情報支援の実現を目指している．これまで,この UD の特徴を活かし室内環境における建物案内システムや投影画像の歪み自動補正,錯視の一つであるアナモルフォーズを用いることで擬似的に立体感を知覚させる裸眼立体投影手法\cite{UD2},知能化空間内のエージェントロボットとして動的環境下において自律的に情報支援を行うための行動モデルの提案\cite{UD5}を行って
きた.

\begin{figure}[t]
\begin{center}
\includegraphics[clip, width=8cm]{figs/ud_with_kinectv2.eps}
\caption{Pictures of Ubiquitous Display}
\label{UD}
\end{center}
\end{figure}

\begin{figure}[t]
\begin{center}
\includegraphics[clip, width=8cm]{figs/demo.JPG}
\caption{Projection information by Ubiquitous Display}
\label{projection}
\end{center}
\end{figure}

\section{関連研究}
本節では，まず，公共空間において人とコミュニケーションを取り，サービスを提供するロボットの「ふるまい」に関する実験や応用例を紹介する．さらに，人工知能技術の一つである深層強化学習が応用されているロボットの例を紹介する．

\subsection{コミュニケーションロボットの行動制御に関する研究}
近年のロボットの制御技術やセンシング技術の発展や，深層学習の応用により，人とのコミュニケーションや高度な動作制御が可能になった．そして，ロボットは，人の労働力の代替だけでなく，人とインタラクションができ，個人に対するサービスの提供で社会に貢献することが期待されている．しかし，公共空間でロボットが個人にサービス提供を行うためには，不特定多数の中から人をひきつける必要があり，人を意識したロボットの「ふるまい」が求められる．

商業施設などの公共の場所において，案内，情報提供，物資運送などのサービス提供を目的として，様々なコミュニケーションロボットを用いた研究が行われている．Tibelらは，空港内での乗り換えを行う人やグループに対して，大勢の人が行き交う空港の中でも，人を目的地まで案内するロボットを開発している\cite{spencr}．都市部の人や障害物が動的に動く環境において，安全なナビゲーションを実現するため，FerreらはExtended Social Force Modelを用いた経路計画を提案している\cite{ESTM}．また，これらの人の支援以外の試みとして，Mizuharaらは，警備員の動きを解析し，ショッピングモールで歩きスマホをしている人に忠告する接近方法を提案している\cite{admonishing}．

\subsection{深層強化学習とロボット}
近年，ロボットや自動運転車の制御などに深層強化学習の技術が活発に使われている．
強化学習とは機械学習の一種であり，ある環境のもとに置かれたエージェントが，現在の状態を観測し，行動に対する報酬を得ながら，試行錯誤を通して報酬を最大化するための行動戦略を学習するための枠組みである\cite{RL}．
深層強化学習は，学習主体に深層ニューラルネットワークを用いて学習する技術で，入力値が連続の状態空間におけるQ学習を可能にした\cite{drl}．現在は，深層強化学習を用いてあらゆる目的を達成するための「ロボットの行動の最適化」が行われている\cite{Human_level_dqn}\cite{sarl}．

%%% 編集中 %%%
深層強化学習を用いた移動ロボットの行動制御の一例として，カメラやLidarから取得したセンサ信号を直接ニューラルネットに入力して「センサからモータまで」制御を行うend-to-endなナビゲーションが多く研究されている\cite{ete}\cite{Self}\cite{vtr}．Leiらは，LIDARから取得したデータからロボットの速度を出力させることで，事前地図が不要な自律移動を提案した\cite{vtr}．また，コミュニケーションロボットの行動制御に関しても深層強化学習が使われている．Xuanらは，サービスロボットが動的な環境下で，人間の位置や動きなどの状態や人間のグループ間の相互作用などの社会的ルールを組み込んだナビゲーションフレームワーク\cite{human_navigation}を，DQNで学習させたモデルで安全にサービスロボットが制御可能なことを報告している\cite{human_navigation_dqn}．

\section{研究目的}
これまで，大型公共施設内の人が行き交う通路を想定したUDの情報支援を目的とした行動モデルは開発されている．しかし，従来の行動モデルの問題点として，情報支援が受けられる人が少ないことや計算量が大きいことが挙げられる．そこで本研究では，これらの問題に対してハードウェアとソフトウェアの両面からアプローチする．まず，ハードウェアに関してはプロジェクタを2個搭載したDual-Ubiquitous Displayを提案する．プロジェクタが1個増加することで，一度に同時に2人の人へ情報支援が可能になり，効率的な情報支援が期待される．そして，ソフトウェアに関しては深層強化学習を用いてUDが最適に情報支援を行うための行動モデルの構築を行う．深層強化学習を使う利点は，システムが深層ネットワークの重みを保持するだけで，最適な行動選択が可能になるため計算量の軽量化が期待される．また，実際にUDが収集した経験から行動を学習するため，複雑な環境であっても目標対象の選別などのタスクを自動で学習することが可能である．本行動モデルの優位性について，シミュレータ実験と実機実験を通して検証する．

\section{論文構成}
本論文の構成を以下に述べる．
第２章では，本研究で開発したロボットについて，ハードウェア構成とソフトウェア構成について述べる．第３章では，本論文で提案する深層強化学習を用いたUDの行動制御手法について述べる．第４章では，提案手法の実験を行い，その結果と考察について述べる．第５章では，本論文のまとめと今後の課題について述べる．


\chapter{Dual-Ubiquitous Display: D-UD}\label{chap:d_ud_mechanism}
本章では，本研究室で研究・開発されてきたUbiquitous Display(以下UD)を拡張し，プロジェクタを2つ搭載したロボットDual-Ubiquitous Display(以下D-UD)の概要について述べる．D-UDが提供できる情報投影手法および設計コンセプトを述べたのち，ハードウェア構成とソフトウェア構成についても詳細に述べる．

\section{D-UDの設計コンセプト}
D-UDはプロジェクタを2台搭載する自律移動型投影ロボットである(Fig.\ref{DUD})．これまで，UDは人が情報端末を所持したり，電子掲示板まで移動したりする必要なく情報を支援することを目標に設計コンセプトを設け開発を行ってきた\cite{UD1}．以下にUDの設計コンセプトの詳細を示す．

\begin{itemize}
\item 実環境全てを網羅する描画領域(ユビキタス表示)
\item 描画情報のシームレスな変化(シームレスな表示)
\item 人の行動・意図に対応するサービス(インタラクティブ表示)
\item Webと連携したコンテンツの表示(適応表示)
\end{itemize}

D-UDはこれらのコンセプトを引き継ぐと共に，異なる位置の人に同時に情報投影できる効率的な情報支援を行うため，全方向四輪移動プラットフォームにパン・チルト回転機構を搭載したプロジェクタ2台から構成されている(Fig.\ref{DUD})．さらに，周囲360°パノラマ画像が取得可能なカメラと距離30m，270°の範囲をデータ出力するレーザスキャナ(LRF)を搭載し，D-UDの活動環境を広範囲に認識することで人の見やすい場所に情報投影を行う人間中心の視覚的情報支援が可能である．また，D-UDは2つのプロジェクタから映像を投影することで，異なる位置の2人へ同時に投影することが可能であり，UDよりも多くの人への効率的な情報支援を行う．さらに，両プロジェクタの映像を重ね合わせて1つの映像にすることで，映像の切り替えが少なく人に優しい情報支援の提供が可能である．Fig.\ref{fig:twofig}はその具体例を示している．まず，D-UDが人を認識し接近を行う(Fig.\ref{fig:lefttop})．ユーザの前まで到着が完了すると，ユーザの「このフロアのマップを見せてください」という問いかけに対して，左側のプロジェクタでフロアのマップを投影する(Fig.\ref{fig:leftbottom})．そして，ユーザの「理研究室の場所を教えて下さい」という問に対して，左側のプロジェクタで投影したマップの上に，右側で目的の場所に関連した付加情報を投影する(Fig.\ref{fig:rightbottom})．

\begin{figure}[t]
\begin{center}
\includegraphics[clip, width=10cm]{figs/dud_overview.eps}
\caption{Overview of Dual Ubiquitous Display}
\label{DUD}
\end{center}
\end{figure}

%\section{D-UDが提供する情報投影手法}

\begin{figure}[htbp]

	\begin{center}
	\subfigure[Approaching human]{\includegraphics[width=6cm]{figs/dud_1_white.eps}
	\label{fig:lefttop}}
	%\hspace{10mm}
	\subfigure[Arrived in front of user]{\includegraphics[width=6cm]{figs/dud_2.eps}
	\label{fig:righttop}}
	\subfigure[Projection image by left projector]{\includegraphics[width=6cm]{figs/dud_3.eps}
	\label{fig:leftbottom}}
	%\hspace{10mm}
	\subfigure[Projection image by right projector]{\includegraphics[width=6cm]{figs/dud_4.eps}
	\label{fig:rightbottom}}
	\end{center}
	
	\caption{Projection flow of Dual Ubiquitous Display}
	\label{fig:twofig}
\end{figure}

%\section{D-UDによる人への情報支援}


\section{ハードウェア構成}
本節では，D-UDを構成するハードウェアの詳細について述べる．
\subsection{コンピュータ}
D-UDの総重量を軽減するために，Intel社のNext Unit of Computing (以後 NUC)を採用した．UDで搭載されたコンピュータの重量が7.0[kg]であるのに対して，NUCは0.4[kg]であり，車輪にかかる負荷を軽減した．Table \ref{table:NUC}にNUCの仕様を示す．

\begin{table}[h]
	\caption{Specifications of Intel NUC}
	\label{table:NUC}
	\centering 
	\begin{tabular}[tbp]{|c|c|}
		\hline 
		External size(WDH) & 115×111×48[mm] \\\hline
		Weight & 0.408[kg] \\\hline
		CPU & Core i5-5250U 1.6[GHz] \\\hline
		Memory & DDR3L 8[GB] \\\hline
	\end{tabular}
\end{table}

% \begin{figure}
% \begin{minipage}{0.4\textwidth}
% \begin{center}
% \includegraphics[scale=0.45]{figs/intel-nuc.eps}
% \caption{Intel NUC}
% \label{NUC}
% \end{center}
% \end{minipage}
% \begin{minipage}{0.5\textwidth}
% \begin{center}
% \makeatletter
% \def\@captype{table}
% \makeatother
% \caption{Specifications of Intel NUC}
% 	\begin{tabular}[tbp]{|c|c|}
% 		\hline 
% 		External size(WDH) & 115×111×48[mm] \\\hline
% 		Weight & 0.408[kg] \\\hline
% 		CPU & Core i5-5250U 1.6[GHz] \\\hline
% 		Memory & DDR3L 8[GB] \\\hline
% 	\end{tabular}
% \label{table:NUC}
% \end{center}
% \end{minipage}
% \end{figure}

D-UDはメインコンピュータであるNUCと，youBotの内部搭載されたコンピュータが通信するとこで１つのシステムとして動作する．この通信は，車輪の姿勢やオドメトリなどのデータの送受信を頻繁に行うため，LANケーブルを用いた有線接続が必要である．一方，外部からD-UDの操作を行う場合，無線接続したコンピュータから操作することが望ましい．そこで，D-UDのメインコンピュータ(NUC)にトラベルルータを接続することで，環境に設置されたWifiに依存することなく，D-UDを無線操作することを可能にした(Fig.\ref{computer_connection})．

\begin{figure}[h]
\begin{center}
\includegraphics[clip, width=12cm]{figs/computer_cinfiguration.eps}
\caption{Computer connection diagram}
\label{computer_connection}
\end{center}
\end{figure}

\subsection{プロジェクタプラットフォーム}
D-UDのプロジェクタプラットフォームにはFixed-Center Pan-Tilt(FC-PT)機構を採用している(Fig.\ref{pantilt})\cite{pantilt}．プロジェクタで投影した映像の線形的な移動をPan-Tilt機構で実現する．パン関節の可動域は125°であり，チルト関節の可動域は280°である．

\begin{figure}[h]
\begin{center}
\includegraphics[clip, width=5.5cm]{figs/pantilt.png}
\caption{Picture of Pan-Tilt mechanism}
\label{pantilt}
\end{center}
\end{figure}

\begin{table}[h]
	\caption{Specifications of Pan-Tilt mechanism}
	\label{table:pantilt}
	\centering 
	\begin{tabular}[tbp]{|c|c|}
		\hline 
		External size(WDH) & 220×350×460[mm] \\\hline
		Weight & 5.5[kg] \\\hline
	\end{tabular}
\end{table}

\begin{figure}[h]
\begin{center}
\includegraphics[clip, width=11cm]{figs/angle_limit.eps}
\caption{Range of motion}
\label{pantilt_range_limit}
\end{center}
\end{figure}

\subsubsection{プロジェクタ}
プロジェクタはLG HF80LSを採用した．Fig.\ref{LG}にLG HF80LSの外観を，Table \ref{table:LG}に仕様を示す．D-UDはプロジェクタを2個搭載する必要があるため，消費電力量の少ないプロジェクタが望ましい．LG HF80LSは，UDで搭載されたプロジェクタ(NEC NP64J)の消費電力が292[w]に対して，140[w]であり約半分の消費電力である．

% \begin{figure}
% \begin{minipage}{0.4\textwidth}
% \begin{center}
% \includegraphics[scale=0.5]{figs/lg_projector.eps}
% \caption{LG HF80LS}
% \label{LG}
% \end{center}
% \end{minipage}
% \begin{minipage}{0.5\textwidth}
% \begin{center}
% \makeatletter
% \def\@captype{table}
% \makeatother
% \caption{Specifications of LG HF80LS}
% 	\begin{tabular}[tbp]{|c|c|}
% 		\hline 
% 		External size(WDH) & 108×252×144[mm] \\\hline
% 		Weight & 2.1[kg] \\\hline
% 		Resolution & 1920×1080 \\\hline
% 		Brightness & 2000[lm] \\\hline
% 		Light source & DLP \\\hline
% 		Power consumption & 140[w] \\\hline
% 	\end{tabular}
% \label{table:LG}
% \end{center}
% \end{minipage}
% \end{figure}

\begin{figure}[h]
\begin{center}
\includegraphics[clip, width=8cm]{figs/lg_projector.eps}
\caption{LG HF80LS}
\label{LG}
\end{center}
\end{figure}

\begin{table}[h]
	\caption{Specifications of LG HF80LS}
	\label{table:LG}
	\centering 
	\begin{tabular}[tbp]{|c|c|}
		\hline 
		External size(WDH) & 108×252×144[mm] \\\hline
		Weight & 2.1[kg] \\\hline
		Resolution & 1920×1080 \\\hline
		Brightness & 2000[lm] \\\hline
		Light source & DLP \\\hline
		Power consumption & 140[w] \\\hline
	\end{tabular}
\end{table}

\subsubsection{パン・チルトモータ}
D-UDではモータとして，Robotis社のDynamixel MX-64ATを使用している．角度制御分解能が高く，円滑な投影画像の移動やぶれの改善が期待できる．MX-64ATの外観をFig.\ref{Dynamixel}に，仕様をTable \ref{table:Dynamixel}に示す．

% \begin{figure}
% \begin{minipage}{0.4\textwidth}
% \begin{center}
% \includegraphics[scale=0.4]{figs/MX_64AT.eps}
% \caption{Dynamixel MX-64AT}
% \label{Dynamixel}
% \end{center}
% \end{minipage}
% \begin{minipage}{0.5\textwidth}
% \begin{center}
% \makeatletter
% \def\@captype{table}
% \makeatother
% \caption{Specifications of Dynamixel MX-64AT}
% 	\begin{tabular}[tbp]{|c|c|}
% 		\hline 
% 		Dimensions (WHD) & 40.2x61.1x41[mm] \\\hline
% 		Weight & 165[g] \\\hline
% 		Resolution & 4096[pulse/rev] \\\hline
% 		Stall Torque & 6.0[Nm] \\\hline
% 		No Load Speed & 63[rev/min] \\\hline
% 	\end{tabular}
% \label{table:Dynamixel}
% \end{center}
% \end{minipage}
% \end{figure}

\begin{figure}[h]
\begin{center}
\includegraphics[clip, width=3cm]{figs/MX_64AT.eps}
\caption{Dynamixel MX-64AT}
\label{Dynamixel}
\end{center}
\end{figure}

\begin{table}[h]
	\caption{Specifications of Dynamixel MX-64AT}
	\label{table:Dynamixel}
	\centering 
	\begin{tabular}[tbp]{|c|c|}
		\hline 
		Dimensions (WHD) & 40.2x61.1x41[mm] \\\hline
		Weight & 165[g] \\\hline
		Resolution & 4096[pulse/rev] \\\hline
		Stall Torque & 6.0[Nm] \\\hline
		No Load Speed & 63[rev/min] \\\hline
	\end{tabular}
\end{table}

\subsubsection{プレート}
D-UDは上部のプレートに2台の雲台を搭載させるため，雲台同士の衝突を回避する必要がある．そこで，雲台同士の距離を広げるため，アルミプレートを作成した(Fig.\ref{plate})．

\begin{figure}[t]
\begin{center}
\includegraphics[clip, width=8cm]{figs/top_plate.png}
\caption{Overview of original top plate}
\label{plate}
\end{center}
\end{figure}

\begin{table}[t]
	\caption{Specifications of original top plate}
	\label{table:plate}
	\centering 
	\begin{tabular}[tbp]{|c|c|}
		\hline 
		External size(WDH) & 650×290×5[mm] \\\hline
		Weight & 1.6[kg] \\\hline
	\end{tabular}
\end{table}

\subsection{センサ}
D-UDは異なる位置の人物に向けて情報支援を行う必要がある．そこで，広範囲の環境を認識するためパノラマカメラと測域センサを使用する．

\subsubsection{カメラ}
D-UDに360°パノラマ画像が取得なRICOH Theta Vを搭載した(Fig.\ref{ThetaV})．Theta Vは2枚の魚眼レンズで撮影した円形画像を，正距円筒図法により360°パノラマ画像に変換するカメラである．D-UDは，UDよりも広範囲な環境認識が必要になるためTheta Vを採用している．Theta Vの仕様をTable \ref{table:ThetaV_TABLE}に示す．

% \begin{figure}
% \begin{minipage}{0.4\textwidth}
% \begin{center}
% \includegraphics[scale=0.3]{figs/theta_v.eps}
% \caption{RICOH Theta V}
% \label{ThetaV}
% \end{center}
% \end{minipage}
% \begin{minipage}{0.5\textwidth}
% \begin{center}
% \makeatletter
% \def\@captype{table}
% \makeatother
% \caption{Specifications of RICOH Theta V}
% 	\begin{tabular}[tbp]{|c|c|}
% 		\hline 
% 		External size(WHD) & 45.2x130.6x22.9[mm] \\\hline
% 		Weight & 121[g] \\\hline
% 		Resolution & 1024×512[pixels] \\\hline
% 		Frame rate & 29.97[FPS] \\\hline
% 		Connection type & Wifi \\\hline
% 	\end{tabular}
% \label{table:ThetaV_TABLE}
% \end{center}
% \end{minipage}
% \end{figure}

\begin{figure}[h]
\begin{center}
\includegraphics[clip, width=5cm]{figs/theta_v.eps}
\caption{RICOH Theta V}
\label{ThetaV}
\end{center}
\end{figure}

\begin{table}[h]
	\caption{Specifications of RICOH Theta V}
	\label{table:ThetaV_TABLE}
	\centering 
	\begin{tabular}[tbp]{|c|c|}
		\hline 
		External size(WHD) & 45.2x130.6x22.9[mm] \\\hline
		Weight & 121[g] \\\hline
		Resolution & 1024×512[pixels] \\\hline
		Frame rate & 29.97[FPS] \\\hline
		Connection type & Wifi \\\hline
	\end{tabular}
\end{table}

\subsubsection{Laser Range Finder}
D-UDに北陽電機のLaser Range Finder(以後 LRF)であるUTM-30LXを2台搭載している(Fig.\ref{hokuyo})．Fig.\ref{DUD}で示したように，D-UDの前後にLRFを取り付けることで，全方位をセンシングすることができる．このLRFは，D-UDの自己位置推定や障害物等との衝突回避に用いられる．Table \ref{table:hokuyo}にLRFの仕様を示す.

% \begin{figure}
% \begin{minipage}{0.4\textwidth}
% \begin{center}
% \includegraphics[scale=0.5]{figs/lrf.eps}
% \caption{Hokuyo UTM30-LX}
% \label{hokuyo}
% \end{center}
% \end{minipage}
% \begin{minipage}{0.5\textwidth}
% \begin{center}
% \makeatletter
% \def\@captype{table}
% \makeatother
% \caption{Specifications of Hokuyo UTM30-LX}
% 	\begin{tabular}[tbp]{|c|c|}
% 		\hline 
% 		External size(WHD) & 60x60x87[mm] \\\hline
% 		Weight & 0.37[kg] \\\hline
% 		Resolution(Distance) & 1.0[mm] \\\hline
% 		Resolution(Angle) & 0.25[deg] \\\hline
% 		Accuracy of measure & \begin{tabular}{c}0.1-10[m]:$\pm$ 30[mm]\\10-30[m]:$\pm$50[mm]\end{tabular}\\ \hline
% 		Scan range(Distance) & 60[m] \\\hline
% 		Scan range(Angle) & 0.25[deg] \\\hline
% 		Scanning time(Distance) & 25[msec] \\\hline
% 	\end{tabular}
% \label{table:hokuyo}
% \end{center}
% \end{minipage}
% \end{figure}

\begin{figure}[h]
\begin{center}
\includegraphics[clip, width=5cm]{figs/lrf.eps}
\caption{Hokuyo UTM30-LX}
\label{hokuyo}
\end{center}
\end{figure}

\begin{table}[h]
	\caption{Specifications of Hokuyo UTM30-LX}
	\label{table:hokuyo}
	\centering 
	\begin{tabular}[tbp]{|c|c|}
		\hline 
		External size(WHD) & 60x60x87[mm] \\\hline
		Weight & 0.37[kg] \\\hline
		Resolution(Distance) & 1.0[mm] \\\hline
		Resolution(Angle) & 0.25[deg] \\\hline
		Accuracy of measure & \begin{tabular}{c}0.1-10[m]:$\pm$ 30[mm]\\10-30[m]:$\pm$50[mm]\end{tabular}\\ \hline
		Scan range(Distance) & 60[m] \\\hline
		Scan range(Angle) & 0.25[deg] \\\hline
		Scanning time(Distance) & 25[msec] \\\hline
	\end{tabular}
\end{table}

\section{ソフトウェア構成}
D-UDの制御には，移動プラットフォームの制御，複数のセンサを使った周囲の環境情報の認識，パン・チルト機構プロジェクタの制御など複数の制御を並列的に処理する必要がある．そこで，本研究ではD-UDの開発において並列処理とコードの再利用性など，ロボット効率的な開発を可能にするミドルウェアのWillow Garage社のROS(Robot Operating System)\cite{ROS}を利用する．

\subsection{ROSミドルウェア}
ROSとは，ロボットソフトウェアプラットフォームの一種である．ROSはノードとよばれるプロセス間で，トピックを介してメッセージを交換することができる．開発者は各プロセスに必要なメッセージの受け渡しを実行することで，容易に各機能の並列処理を行うことができる．D-UDのソフトウェア制御では，各ノードがパブリッシュ・サブスクライブしデータを送受信することで，すべてのプログラムの並列処理を可能にしている．D-UDに実装したコアとなるノード群をFig.\ref{rqt}に示し，各ノードの機能説明をTable \ref{table:ros_node}に示す\footnote{https://github.com/MickeyKen/dual\_ubiquitous\_display\_core}．コアノード群は，アクチュエータやセンサの起動からキーボードやjoystickを用いたD-UDのマニュアル操作までD-UDの基本的な機能を提供する．また，D-UD全体の座標管理やLidarの距離情報を用いたナビゲーション，プロジェクタの投影面の歪み計算も実装されている．そのため，開発者は各ノードに必要なデータの通信を考慮するだけで，容易にD-UDのシステムの拡張を行うことができる．

%\ref{chap:control}節にD-UDのコアとなる制御パッケージについて述べる．また，\ref{chap:recognition}節でカメラやLidarを用いた環境認識プログラムの構成について述べる．

\begin{figure}[t]
\begin{center}
\includegraphics[clip, width=18cm, angle=90]{figs/dud_rqt_graph.eps}
\caption{Nodes and topics}
\label{rqt}
\end{center}
\end{figure}

\begin{table}[t]
\begin{center}
\caption{Name and function of each node in the core package}
\label{table:ros_node}
\scalebox{0.9}[0.9]{
  \begin{tabular}{|c|p{8cm}|}\hline
   ノード名 & 機能説明 \\\hline
   joy\_node & joystickの入力信号を受信して，トピックに変換するノード． \\ \hline
   joy\_teleop\_node & joy\_nodeからパブリッシュされたトピックを受信し，youBotの車輪とパン・チルト機構のモータを制御するトピックをパブリッシュするノード． \\ \hline
   keyboard\_teleop\_node & キーボードの入力信号を受信して，youBotの車輪とパン・チルト機構のモータを制御するトピックをパブリッシュするノード． \\ \hline
   youbot\_driver & 速度のトピックをサブスクライブし，youBotの車輪制御を行うノード． \\ \hline
   move\_base & 目標位置と現在位置のトピックをサブスライブして，経路計画とナビゲーションを実行するノード． \\ \hline
   amcl & Lidarのセンサ値をサブスクライブして，自己位置推定を行うノード． \\ \hline
   map\_server & 事前に作成したマップの画像情報をパブリッシュするノード． \\ \hline
   joint\_states\_publisher & パン・チルト機構のモータやyouBotの車輪の各関節角度を追従し，パブリッシュするノード． \\ \hline
   robot\_states\_publisher & joint\_states\_publisherがパブリッシュするトピックをサブスクライブし，URDFモデルと照合した後ロボットの姿勢のトピックをパブリッシュするノード． \\ \hline
   dynamixel\_workbench\_controllers & パン・チルト機構のモータの目標角度をサブスライブし，Dynamixelを位置制御するノード． \\ \hline
   front\_lidar & D-UDの前方のHokuyo UTM-30LXのセンサ値をパブリッシュするノード． \\ \hline
   rear\_lidar & D-UDの後方のHokuyo UTM-30LXのセンサ値をパブリッシュするノード． \\ \hline
   merge\_lidar\_node & front\_lidarとrear\_lidarからパブリッシュされたトピックをサブスクライブし，それらを統合した後パブリッシュするノード． \\ \hline
   left\_projection\_node & robot\_states\_publisherからパブリッシュされたトピックを受信し，パラメータから取得した画像・動画名を歪み補正して左側のプロジェクタから投影するノード． \\ \hline
   right\_projector\_node & robot\_states\_publisherからパブリッシュされたトピックを受信し，パラメータから取得した画像・動画名を歪み補正して右側のプロジェクタから投影するノード． \\ \hline

  \end{tabular}
}
  \end{center}
\end{table}

\subsection{ロボットモデリングとシミュレーション}
ROSでは，ロボット制御のための視覚化ツールとシミュレーションツールが存在する．これらのツールを利用することで，ロボットの実機を制御するプログラムを開発する際，計算機上で物理計算をシミュレーションすることで，手軽に事前の動作確認が可能となり開発効率が改善される．本研究でも，シミュレーションを活用するためD-UDのURDF(Unified
Robot Description)モデルの作成した(Fig.\ref{fig:left})．Fig.\ref{fig:right}はRViz(ROS Visualization)という3次元視覚化ツールを用いてD-UDのURDFモデルを表示している．また，シミュレーションツールにはGAZEBO\cite{GAZEBO}を採用した．GAZEBOは様々なロボットセンサをモデル化するだけでなく，実世界の剛体物理もシミュレーション環境中でモデル化できる3Dロボットモデルシミュレータである．

\begin{figure}[tbp]
	\begin{center}
	\subfigure[Simulation of Dual Ubiquitous Display main body in GAZEBO]{\includegraphics[height=4cm]{figs/dud_gazebo.eps}
	\label{fig:left}}
	\hspace{5mm}
	\subfigure[URDF model of Dual Ubiquitous Display in Rviz]{\includegraphics[height=4cm]{figs/dud_urdf.eps}
	\label{fig:right}}
	\subfigure[Dual Ubiquitous Display 3D model in the GAZEBO environment. Blue area indicates the range of LIDAR scanning.]{\includegraphics[width=9.8cm]{figs/dud_gazebo_environment2.eps}
	\label{fig:bottom}}
	\end{center}
	\caption{Robot Modeling in GAZEBO/ROS}
	\label{fig:Modeling}
\end{figure}

\chapter{深層強化学習を用いた \\ UDの行動制御}\label{chap:proposal}
第\ref{chap:introduction}章で述べた課題に対する提案手法を詳しく説明する．
本章では，深層強化学習の技術の1つであるDQN(Deep Q-Network)を用いたUDの行動制御について述べる．まず，DQNの概要とネットワークの学習過程の効率化に関する技術について説明する．そして，それらの技術を用いたUDが人に情報支援のための行動制御を学習する手法の概要を述べる．

\section{Deep Q-Network(DQN)の概要}
Deep Q-Network (DQN)は，強化学習の手法の一つであるQ学習と，深層ニューラルネットワークを組み合わせて学習させる技術である\cite{drl}． Q学習では，学習主体であるエージェントが状態sで行動aをとったときの行動価値をQ値という値で評価し，ある状態においてQ値が高い行動を選択する方策を学習していく．状態$s_{t}$における行動$a_{t}$のQ値の更新は，以下の式

\begin{equation}
\label{q_update}
Q(s_{t},a_{t} )←Q(s_{t},a_{t} )+ α_{t} (r_{t}+ \gamma maxQ(s_{t+1},a)-Q(s_{t},a_{t}))
\end{equation}

によって更新される．ここで，$r_{t}$はスッテプ$t$での報酬であり，$\gamma$は割引率と呼ばれる$ 0 < \gamma <1 $を満たす定数である．
また，$a_{t}$は学習係数であり，学習の速度を決定する定数である．$maxQ(s_{t+1},a)$は状態$s_{t+1}$における可能な行動の中の最大価値を示している．Q学習はエージェントにQ-tableを用いてQ値の更新を行うが，DQNは学習主体に深層ニューラルネットワークを用いてQ値の更新を行う手法である(Fig.\ref{DQN})．深層ニューラルネットを用いることで，センサ値やロボットの関節角などの状態空間を連続値として入力するが可能となり，学習が収束しやすくなる．

%しかし，エピソードの時系列順で学習を行うと，連続する状態の前後の相関が強いため，学習に偏りが生じる問題がある．そこで，Experience Replayと呼ばれる技術を用いて学習の安定化を図った．Experience Replayは過去の経験${\{s_{t},a_{t},r_{t},s_{t+1}\}}$の系列データをメモリに一度貯めておき，Q関数を推定する際にそれらから無作為に抽出する手法である．

\begin{figure}[t]
\begin{center}
\includegraphics[clip, width=9cm]{figs/dqn_structure.eps}
\caption{Deep Q-Network structure}
\label{DQN}
\end{center}
\end{figure}


\section{DQNの効率的な学習手法に関する研究}\label{DQNの問題}
深層強化学習を用いた行動最適化は，様々な分野ですぐれた性能を発揮している．しかし，それらの成果物を得るためには，長時間の学習を必要とする．Hasselらは，拡張したDQNを用いてAtariに収録されているゲームをクリアするためには10日の学習日数が必要であると報告している\cite{rainbow}．また，学習時間の問題はロボットの分野でもより深刻な問題となっている．ロボットが環境と相互作用し経験を取得し学習することは，現実世界でもシミュレーション環境においてもゲームよりも複雑な処理をするからである．Zhenらは，物理シミュレーションであるGAZEBO\cite{GAZEBO}の長時間の利用が，処理時間の遅延を引き起こすため，並列処理を用いたロボットの自律移動の学習方法を提案している\cite{pararell}．このように，学習時間を短縮する工夫は，様々な発展を遂げている．よって本章では，学習時間の短縮を目指した学習の安定化・効率化および分散学習の技術について述べる．

\subsection{Double Deep Q-Network (DDQN)}
DQNは，最適な行動$a$を選択するネットワークと行動価値を評価するネットワークを同一のモデルで学習を行う．つまり，式(\ref{q_update})で示したとおり，次の状態の最大のQ値を求めるときに自身のネットワークを用いて計算している．しかし，ネットワークのパラメータ更新時に$Q(s,a)$の上方向の誤差が生じるため，行動$a$を過大評価してしまう恐れがある．そこで，Hasseltらは行動選択のネットワークと評価するネットワークを分けて学習を行うDDQNを提案している\cite{double}．パラメータ更新に必要なTD誤差の計算において，$maxQ(s_{t+1},a)$の計算を行うネットワークを$Q_{main}$とし，その行動の評価を行うネットワークを$Q_{target}$とするとQ値の更新式は以下のように書ける．

\begin{equation}
\label{a_main}
a_{main}=argmaxQ_{main}(s_{t+1},a)
\end{equation}

\begin{equation}
\label{double_q_update}
Q(s_{t},a_{t} )←Q(s_{t},a_{t} )+ α_{t} (r_{t}+ \gamma maxQ_{target}(s_{t+1},a_{main})-Q(s_{t},a_{t}))
\end{equation}

\subsection{Multi-step Learning}
Multi-step Learningは行動修正を行う際，複数ステップを考慮する手法である\cite{rl_book}．強化学習において，行動の修正の方法は，実績から修正を行うMonte Carlo法と予測して修正を行うTD法(Temporal Difference Learning)の2つに分かれる．前者のMonte Carlo法の長所は，実際に獲得した即時報酬の総和から行動を修正することによる正確な行動修正技術である．しかし，エピソードが終了するまで修正することができない問題点がある．後者のTD法はエピソード終了までの合計報酬を予測しながら行動を修正するため，エピソードの途中でも行動修正が可能である．しかし，予測に基づくため正確性に欠ける問題がある．Multi-step Learning法はこれらの手法の中間に位置する手法である．よって，式\ref{multi_step}のように合計報酬の式は，即時報酬とnステップ先までの予測を合算し価値を算出する．

\begin{equation}
\label{multi-step}
G_{t}=R_{t+1}+\gamma R_{t+2}+\cdots+\gamma^{n-1}R_{t+n}+\gamma^n Q(s_{t+1},argmaxQ(s_{t+n},a)) \label{multi_step}
\end{equation}

$\gamma(0<\gamma<1)$は割引率であり即時報酬獲得の不確かさを表す係数である．

\subsection{Prioritized Experience Replay}
Prioritized Experience Replay(PER)は，TD誤差の大きい経験から優先してサンプリングを行う手法である\cite{prioritized}．TD誤差の大きい経験は，高報酬を得たものや，非常に悪い行動を選択したものなど学習効率が高いものが多く，学習を進めるにあたって重要なサンプルとなる．TD誤差$\delta_{j}$は以下のように定義される．
\begin{equation}
\label{TD_error}
  \delta_{j}=r(s_{t},a_{t})+\gamma Q^{\prime}(s_{t+1},a_{t+1},w)-Q(s_{t},a_{t},w)
\end{equation}

しかし，グリーディにTD誤差の優先度を反映しサンプリングする場合，最初にTD誤差が小さくなった経験は，リプレイされずに長時間放置されるといった問題がある．また，TD誤差が大きいものばかり優先されることで，経験の多様性が失われ過学習につながるおそれがある．そこで，確率的サンプリング手法を用いて確率的に選択する手法が提案されている．
\begin{equation}
\label{sum_P_i}
  P(i)=\frac{D_{i}^{\alpha}}{\sum_{k}D_{k}^{\alpha}} \label{P_i}
\end{equation}

式\ref{P_i}において，$D_{i}=\frac{1}{rank(i)}>0$と定義され，$rank(i)$はリプレイブッファ内の経験$i$の絶対値の大きさの順位を示す．また，PERはprioprityに基づく確率により，replayに使うデータをサンプリングすることによりデータの分布を変えてしまうため，推定している期待値のバイアスが大きくなる．そこで，このバイアスを是正するために，サンプリングされる確率の大きいデータほど，lossへの寄与を小さくし，重要度重み(Important Sampling Weights)を経験に掛けてデータに補正をかける．

\subsection{深層強化学習の分散アーキテクチャ}
エージェントの機能を分散させることでサンプル効率を上げて学習する仕組みが近年盛んに研究されている\cite{A3C}\cite{Gorila}\cite{Ape-X}．Mnihらは分散環境を使用した方策勾配法による深層強化学習の手法を提案している\cite{A3C}．A3C(Asynchronous Advantage actor-critic)は，各エージェントでデータ収集だけでなく学習も行う．しかし，各エージェントで学習した重みを共有する場合，常に最新の学習結果が求められることや通信の部分でボトルネックとなっていた．そこで，多数の探索を行うActorと学習器を分離し，各エージェントが経験の収集のみを行う手法が提案された\cite{Gorila}\cite{Ape-X}．

\section{提案手法の概要}
\subsection{UDが情報支援を行うための前提条件}\label{投影支援条件}
UDが人に快適かつ効果的に情報投影を行うためには，人の視野角や近接学に基づいたUDの立ち位置の決定が必要である．また，UDはプロジェクタの光で情報を投影するため，投影位置から距離が遠すぎる場合は光が十分に届かず，逆に近すぎる場合は適切な大きさの情報を投影できない問題がある．そこで，本章ではこれらの問題を考慮したUDの立ち位置決定の条件に関して詳細に述べる．

\begin{itemize}
 \item ロボットの存在感のアピール
    
    ロボットの商品の販売促進活動実験\cite{販売促進}では，ロボットの姿が見えるときと見えないときで販売数の結果に差が出ており，ロボットの外観が持つ販売促進効果が報告されている．したがって，UDが投影した情報を効果的にユーザへ注目させるためには，情報投影時にユーザが有効視野及び注視安定視野のいずれかの範囲内に存在することが望ましい．有効視野とは，眼球の動きのみで情報を見ることが可能で，瞬時に特定情報を雑音内より把握できる範囲(水平方向に30度)である．また，注視安定視野とは，頭部運動が眼球運動を助ける状態で発生し，無理なく注視が可能な範囲(水平方向に30度〜90度)である．よって，ユーザの水平方向90度以内をUDが効果的に情報投影できる範囲とする．
    
 \item ユーザへの危機感・不快感の除去
 
    ロボットが人にサービスを行う際，人とロボット間の距離が不適切な場合，ユーザに不快感を与える可能性がある．Hallらは人同士が会話するときの距離を4つのクラスに分類した\cite{Hall}．ロボットと人同士においても同様にインタラクション時の適切な距離に関する研究が行われている\cite{proximity}．また，会話の場面に限らず，人は無意識のうちにパーソナルスペースと呼ばれる縄張りのような空間が存在する．本研究では，これらの知見をもとに人の0.5[m]以内への侵入を禁止し，危機感・不快感を与えない距離とする．
 
 \item 投影情報の最低解像度の保障
 
    プロジェクタが投影位置と離れるほど実環境における投影画像の解像度が下がる．本実験では，ユーザが快適に視認できる映像を投影できる範囲を2500[m]に設定した．また，UDはハードウェアの構造から床面に投影する際，12000[mm]以内に投影することはできない．さらに，1500[mm]以内では，映像が欠けてしまい1.0[m]×1.0[m]の映像を投影することができない．よって，投影位置から1500[mm]〜2500[m]を投影情報の最低解像度が保障される距離とする．
\end{itemize}


\subsection{GAZEBOとROSを用いた分散アーキテクチャ}
\ref{DQNの問題}節で述べた学習時間の短縮する仕組みとして，本研究ではGAZEBOとROSを用いた分散学習を導入した．本研究において，エージェントの機能を経験を収集するActorと経験を保存するMemory，Memoryのデータを用いてネットワークの学習を行うLearnerに分離した(Fig.\ref{agent_architecture})．また，経験データの収集プロセスを並列化し収集効率を上げるため多数のActorを設けた．しかし，多数のActorを用いて探索を行う場合，各Actorにつき環境が必要になるため複数のシミュレーション環境を並列して起動しなければならない．そこで，ROSの機能を拡張し分散アーキテクチャを構築した．ROSはマスタ(master)というノード間の接続とメッセージ通信の実現に必要なサーバ（ネームサーバ）を持つ．しかし，1つのマスタにつき同名のノードを複数登録することはできない．よって，本研究では異なるROSマスタを設定し，複数の独立したROSネットワークを構築する方法を採用した．ROSマスタとノード間の通信はXML-PRCを用いて行われ，環境変数の「\$ROS\_MASTER\_URI」と「\$GAZEBO\_MASTER\_URI」により通信ポートが決定される．そのため，任意のアドレスに変更するためには，ターミナルで以下のコマンドを実行する．

\begin{lstlisting}[basicstyle=\ttfamily\footnotesize, frame=single]
export ROS_MASTER_URI=http://localhost:11350
export GAZEBO_MASTER_URI=http://localhost:11340
\end{lstlisting}

Fig.\ref{fig:lefttop_dqn}は単一エージェントのアーキテクチャの図を示し，Fig.\ref{fig:righttop_dqn}は，分散学習を用いた提案するアーキテクチャの全体図を示している．Actorは複数のスレッドから構成されており，各スレッドにWorkerを持つ．各Workerは環境から遷移データを収集し，学習する機能を持つLearnerに遷移データを送信する．LearnerはActorから収集した遷移データをPrioritized Experience Replayでサンプリングし，それらをもとに学習を行う．そして，Actorがパラメータ更新の命令を出すとLearnerのネットワークのパラメータと同期する．上記の処理を繰り返し行うことで学習を進める仕組みである．

\begin{figure}[t]
\begin{center}
\includegraphics[clip, width=10cm]{figs/apex_agent_structure.eps}
\caption{Agent architecture}
\label{agent_architecture}
\end{center}
\end{figure}

\begin{figure}[tbp]

	\begin{center}
	\subfigure[single Agent]{\includegraphics[width=3.5cm]{figs/single_agent.eps}
	\label{fig:lefttop_dqn}}
	\hspace{10mm}
	\subfigure[Multi agent]{\includegraphics[width=7.5cm]{figs/multi_agent.eps}
	\label{fig:righttop_dqn}}
	\end{center}
	
	\caption{Figure of agent architecture}
	\label{fig:twofig_dqn}
\end{figure}


複数エージェントで学習する利点は，学習時間の短縮の他に，多様な遷移データの生成がある．エージェントは自らの行動から状態の遷移および報酬を獲得し調査を行う.調査とは,行動価値に基づいて行動を決定するのではなく,行動価値とは無関係に一様に任意の行動を選択することである．これは，局所的な解に陥ることを防ぐ役割を持ち「探索」と呼ばれる．一方，行動価値を優先し報酬目的に行動を選択することは「活用」と呼ばれる．$\epsilon-greedy$法は，探索と活用のトレードオフのバランスをとる手法の一つである．行動選択の際，乱数を生成しその値が$\epsilon$以下の場合にランダムに行動を選択し，それ以上の場合に行動価値の大きい行動を選択する．通常単一エージェントで学習する場合，$\epsilon$の値は一つしか設定することが出来ない．しかし，複数エージェントで学習する場合，各エージェントに異なる$\epsilon$の設定が可能である．その結果，収集した遷移データに多様性が生まれ，学習効率の向上が実現する．本研究では，$i$番目のActorが持つ$\epsilon$の値を以下の式で求めている．

\begin{equation}
\label{sum_P_i}
  \epsilon_{i}=\epsilon^{1+\frac{i}{N-1}\alpha}
\end{equation}

$N$はActor数を表し，$\epsilon=0.4$，$\alpha=5$とした．

\subsection{DQNを用いた行動制御}
UDが人へ情報を支援するための行動は，プロジェクタの姿勢を自由に回転できるパン・チルト機構を構成するアクチュエータと移動プラットフォームの車輪を適切に制御することで実現する．よって，パン・チルト関節のアクチュエータの目標角度と車輪速度を出力とするディープニューラルネットワークモデルのアーキテクチャを構築した(Fig.\ref{structure})．本モデルは中間層が3層あり入力層から出力層までの全ての層間を全結合層で結合している．そして，入力値に9ノード，出力層に8ノードを持つ．入力値は，UDのパン・チルト関節の現在角度，車輪の現在直進速度及び，車輪の移動方向から水平角度30度内で取得したLidarの最小値2点である．また，それらに加えて目標人物位置と目標投影位置に関する情報を入力している．入力値としては，UDとそれらの位置がなす直線距離$d_{h},d_{p}$となす角度$\theta_{h},\theta_{p}$
である．出力される行動群の詳細を以下に示す．

\begin{itemize}
    \item $action1$ - No operation 
    \item $action2$ - Stop ($v_{linear} = 0.0 m/s$)
    \item $action3$ - Move Forwward ($v_{linear} = +2.0 m/s$)
    \item $action4$ - Move Backword ($v_{linear} = -2.0 m/s$)
    \item $action5$ - Rotate pan joint clockwise
    \item $action6$ - Rotate pan joint counterclockwise
    \item $action7$ - Rotate tilt joint clockwise 
    \item $action8$ - Rotate tilt joint counterclockwise
\end{itemize}

\begin{figure}[t]
\begin{center}
\includegraphics[clip, width=11cm]{figs/network_structure.eps}
\caption{Figure of network structure}
\label{structure}
\end{center}
\end{figure}

\begin{figure}[t]
\begin{center}
\includegraphics[clip, width=10cm]{figs/input_lrf_data.eps}
\caption{Figure of input lrf data}
\label{input_lrf}
\end{center}
\end{figure}

\begin{figure}[t]
\begin{center}
\includegraphics[clip, width=7cm]{figs/input_human_network.eps}
\caption{Angle and distance between UD and human}
\label{input_term}
\end{center}
\end{figure}

報酬設計はUDが情報投影に成功した場合に$r_{projection}$で正の報酬を与え，人や壁などの障害物に衝突した場合に$r_{collision}$で罰則として負の報酬を与える．罰則を与えることにより，人や壁への衝突を回避する行動獲得が可能となり，現実世界においても安全にUDが情報投影する行動の実現を目指す．投影成功の条件は，UDが現在投影している位置($c_{p}$)と目標投影位置間の距離($dist(c_{p},d_{p})$)が$TH_{collision}$以下の誤差になり，\ref{投影支援条件}項で述べた条件内の位置でUDが停止している状態($v_{linear}=0$)で投影成功とみなす．これは，UDが動きながら情報投影を行うと，車輪の振動で投影映像が小さく振動し人に不快感を与える可能性があるためである．また，$min_{x_t}$はlidarで取得したセンサ値であり，障害物との衝突を判定している．

\begin{equation}
r(s_{t},a_{t})= \left \{
\begin{array}{l}
r_{projection}\,if\,dist(c_{p},d_{p})<TH_{collision}\,and\,v_{linear}=0 \\
r_{collision}\,if\,min_{x_{t}} < c_{o} \\
\end{array}
\right.
\end{equation}


\chapter{実験}
\section{シミュレーションを用いた実験}
本モデルの有用性を示すため，動的シミュレータであるGazeboを用いて動作実験を行った．

\subsection{実験目的と環境}
Fig.\ref{gazebo}に実験のフィールドを示す．$6400[mm]$×$8500[mm]$の壁に囲まれた部屋内で，ランダムな位置に出現する直立状態の人へ情報投影するための行動獲得を目指した．また，UDがプロジェクタから投影した情報を赤い丸で示している．1エピソードを150回の試行とし，単一エージェントを用いたDQNと分散エージェントを用いたDQNでそれぞれ5000回エピソードを学習させた．エピソード終了の条件は，目標人物の前方2.5m先に情報を投影する，障害物に衝突する($c_{o}=800[mm]$)，150ステップが経過するの3条件である．そして，各報酬値は$r_{projection}$を150，$r_{collision}$を-200で設定し，それ他の状態では-1の報酬を与えた．さらに，投影成功や衝突などのイベントがなく150ステップを経過した場合は，追加で-50の罰則を設定した．実験に使用したコンピュータの構成をTable \ref{table:conmupter_experiment}に示す．また，学習時のパラメータの設定値をTable \ref{table:list_hyperpaeameters}に示す\footnote{実験用スクリプト: https://github.com/MickeyKen/distributed\_dqn}．


\begin{figure}[t]
\begin{center}
\includegraphics[clip, width=12cm]{figs/experiment_environment.eps}
\caption{Simulation environment}
\label{gazebo}
\end{center}
\end{figure}

\begin{table}[t]
	\caption{Hardware information}
	\label{table:conmupter_experiment}
	\centering 
	\begin{tabular}[tbp]{|c|c|}
		\hline 
		OS & Ubuntu 18.04.5 LTS (64bit) \\\hline
		CPU & Intel Core i7-6700K (4.00GHz$\times$8) \\\hline
		Memory & 16GB \\\hline
		GPU & GeForce GTX TITAN X (12GB) \\\hline
	\end{tabular}
\end{table}

\begin{table}[t]
	\caption{List of hyperparameters}
	\label{table:list_hyperpaeameters}
	\centering 
	\begin{tabular}[tbp]{ll}
		\hline 
		minibatch size & 512 \\
		replay memory size & 200000 \\
		steps & 150 \\
		discount factor & 0.99 \\
		multi-step size & 3 \\
		target update interval & 2500 \\
		optimizer & RMSProp
		
	\end{tabular}
\end{table}

\subsection{実験結果}
5000エピソードの学習結果をFig.\ref{result_reward}に示す．Fig.\ref{result_reward}は，縦軸は100エピソード毎の合計報酬の平均値を示し，横軸はエピソード数を示している．エージェントの数に関わらず学習が進むにつれ合計平均報酬が増加し行動の最適化ができた(Fig.\ref{result_loss})．また，合計平均報酬が100を超えるまでに，DQNで学習した場合は26時間かかり，分散学習を用いたDQNで学習した場合は7時間かかった．これは，エージェントの数の問題だけでなく，$\epsilon$の設定やPrioritized Experience Replayによる学習の効率化ができたと考えられる．Fig.\ref{result_reward}より，約800エピソード前後までは合計報酬が負の値になっていることが多く，壁に衝突したり，情報投影が成功しないまま150ステップが経過している状態が観測された．しかし，800エピソード前後からは，合計報酬が50を上回り，1000エピソード付近では約100ステップ以内で情報投影が完了している．また，Fig.\ref{result_probability}は100エピソード毎に発生した3つのイベントの確率を示してる．イベントは投影成功(success)，障害物との衝突(collision)，150ステップ以内にエピソード終了(past 150 steps)である．さらに，Fig.\ref{result_q_value}は行動価値であるQ値の最大値を示している．Q値の最大値は学習が進むにつれ増加しているため，正の報酬(投影成功)に向けた行動が選択出来ていることを表している．したがって，本モデルは情報投影の行動獲得を実現している．Fig.\ref{sim_experiment_motion}は学習後のモデルを用いてUDが人へ情報支援をしている様子である．左グラフは8つの行動のQ値を表しており，UDはQ値の最も高い行動を選択し実行し続けることで，\ref{投影支援条件}項で述べた範囲内の立ち位置から目標人物の2.5m先方へ情報投影ができている．

\begin{figure}[t]
\begin{center}
\includegraphics[clip, width=11cm]{figs/result_multi_reward.eps}
\caption{Simulation result of average reward over the 100 last episodes}
\label{result_reward}
\end{center}
\end{figure}

\begin{figure}[t]
\begin{center}
\includegraphics[clip, width=11cm]{figs/result_multi_loss.eps}
\caption{Simulation result of losses}
\label{result_loss}
\end{center}
\end{figure}

\begin{figure}[t]
\begin{center}
\includegraphics[clip, width=11cm]{figs/result_multi_probability.eps}
\caption{Simulation result of event probability}
\label{result_probability}
\end{center}
\end{figure}

\begin{figure}[t]
\begin{center}
\includegraphics[clip, width=11cm]{figs/result_multi_q_value.eps}
\caption{Simulation result of MAX Q-value}
\label{result_q_value}
\end{center}
\end{figure}

% \begin{figure}[t]
% \begin{center}
% \includegraphics[clip, width=13cm, angle=0]{figs/experiment_motion.eps}
% \caption{Result of UD motion}
% \label{experiment_motion}
% \end{center}
% \end{figure}

% \begin{figure}[tbp]
% 	\begin{center}
% 	\subfigure[Results of reward]{\includegraphics[width=11cm]{figs/result_multi_reward.eps}
% 	\label{fig:result_reward}}
% 	%\hspace{5mm}
% 	\subfigure[Results of loss]{\includegraphics[width=11cm]{figs/result_multi_loss.eps}
% 	\label{fig:result_loss}}
% 	\subfigure[Results of MAX Q-value]{\includegraphics[width=11cm]{figs/result_multi_q_value.eps}
% 	\label{fig:result_q_value}}
% 	\end{center}
% 	\subfigure[Results of event probability]{\includegraphics[width=11cm]{figs/result_multi_probability.eps}
% 	\label{fig:result_probability}}
% 	\caption{Graph of result}
% 	\label{fig:result_graph}
% \end{figure}

\subsection{考察}
シミュレーションによる動作実験では，人の位置や投影位置情報などからUDが情報投影するための行動を獲得できた．Fig.\ref{sim_experiment_motion}より，獲得した行動はエピソードの最初のステップで車輪の速度に関する行動を選択し，移動している間にパン・チルト関節に関する行動の出力を確認した．これは，1ステップが経過する毎に負の報酬($r_{other}=-1$)が与えられるため，出来る限り最短のステップでエピソードを終了させるためであると考えられる．また，UDが投影可能な立ち位置へ移動するまで，Q値がすべて負の値であることから，UDの立ち位置や車輪移動に対する報酬設計を改善する必要がある．

\begin{figure}[t]
\begin{center}
\includegraphics[clip, width=13cm]{figs/sim_motion.eps}
\caption{The motion generated by DQN algorithm in GAZEBO}
\label{sim_experiment_motion}
\end{center}
\end{figure}

\clearpage

\section{実機を用いた実験}
実世界環境において，シミュレーション実験で獲得した情報支援動作の実現を目指して検証実験を行った．

\subsection{実験環境と方法}
シミュレーション実験で獲得した学習済みのモデルを用いて，実環境でUDの実機による情報支援の動作検証した．実験環境は立命館大学BKC内にあるクリエーションコア2階の試写室($6400[mm]$×$8500[mm]$)で行った．シミュレーション環境では，目標人物の位置や向きはパラメータとして取得可能であったが，実世界ではセンサなどを用いてそれらの情報を認識する必要がある．そこで，UDの前方に広範囲の画像情報が取得できる魚眼カメラを新たに設置した(Fig.\ref{ud_with_fisheyecamera})．\ref{人検出スタック}項で魚眼カメラと2D LRFを組み合わせた人物検出のアルゴリズムについて述べる．

\subsection{2D LRFと魚眼カメラを用いた人物の位置・向き検出}　\label{人検出スタック}
実機実験において，目標人物の検出を2D LRF(以下: LRF)と魚眼カメラを用いて行った．魚眼カメラは180度のRGB情報が取得可能なELP社のELP-USB8MP02G-L180を採用した．画像の解像度は1600×1200画素である．また，LRFは北陽電機社のUTM-30LXであり,仕様をTable\ref{table:hokuyo}を示す.

\begin{figure}[t]
\begin{center}
\includegraphics[clip, width=7cm]{figs/ud_with_fisheyecamera.eps}
\caption{Ubiquitous Display with fisheye camera}
\label{ud_with_fisheyecamera}
\end{center}
\end{figure}

本システムはRGB画像情報から目標人物の検出を行った後，検出した目標人物の矩形内にあるLRFのセンサ値を用いて，脚と顔の向きから目標人物の立ち位置と向きを推定する．RGB画像による人物の検出には，深層学習をアルゴリズムに持ち様々な物体を検出可能なIntel OpenVinoパッケージ\footnote{https://github.com/intel/ros\_openvino\_toolkit}を利用した．また，LRFによる脚検出には，ROS公式のpeopleスタック\footnote{https://github.com/wg-perception/people}の一部であるleg\_detectorノードを用いた．

本システムの処理の流れの概要図をFig.\ref{flow}に示す．LRFは周囲270度にある障害物までの距離を計測可能であり，そのセンサ値から人物の脚検出と人物位置の推定が可能である．しかし，距離情報だけを用いると，机の脚など人の脚と似た特徴の物体を誤検出する恐れがある．そこで，画像情報による人物検出を導入した．まず，LRFと魚顔カメラの設置位置が異なるため，両者のセンサ値をフュージョンする必要があった．そこで，内部パラメータが既知の魚眼カメラの二次元座標とLRFで検出した三次元座標の組から，PnP問題(Perspective n-Point Problem)を解き魚眼カメラの外部パラメータを求めた．また，式\ref{transformation}は，カメラの内部パラメータと外部パラメータが既知であるとき，3次元座標を画像内の二次元座標に変換できることを示している．Fig.\ref{reprojection}はLRFで検出した3次元座標を画像に投影している様子である．

\begin{figure}[t]
\begin{center}
\includegraphics[clip, width=10cm]{figs/flow_human_estimation_stack.eps}
\caption{System flow of Human pose stack}
\label{flow}
\end{center}
\end{figure}

\begin{eqnarray}
s\left[
\begin{array}{ccc}
u \\
v \\
1 \\
\end{array}
\right]
=
\left[
\begin{array}{ccc}
f_{x} & 0 & c_{x} \\
0 & f_{y} & c_{y} \\
0 & 0 & 1 \\
\end{array}
\right]
\left[
\begin{array}{cccc}
r_{11} & r_{12} & r_{13} & t_{1} \\
r_{21} & r_{22} & r_{23} & t_{2} \\
r_{31} & r_{32} & r_{33} & t_{3} \\
\end{array}
\right]
\left[
\begin{array}{c}
X \\
Y \\
Z \\
1 \\
\end{array}
\right] \label{transformation}
\end{eqnarray}

%\begin{eqnarray}
%\left[
%\begin{array}{c}
%x \\
%y \\
%z \\
%\end{array}
%\right]
%=R
%\left[
%\begin{array}{c}
%X \\
%Y \\
%Z \\
%\end{array}
%\right]
%+t
%\end{eqnarray}

\begin{figure}[t]
\begin{center}
\includegraphics[clip, width=7cm]{figs/reprojection.eps}
\caption{Projected 2D LRF points}
\label{reprojection}
\end{center}
\end{figure}

前述したように，本システムは画像から人物検出した後，検出範囲にあるLRFのセンサ値を用いて両脚の位置を推定する．しかし，OpenVINOによる物体検出の出力は，検出した物体の矩形の位置座標であるため，両脚部分だけでなく背景部分のセンサ値も含まれる．そこで，人物と背景のセンサ値を分離させるため，k-means法によるクラスタリング処理を行った．k-means法は非階層型のクラスタリング手法であり，ランダムに配置されたクラスタ中心の中から各個体は最も近い中心に割り当てられる．そして，クラスタごとに中心位置が収束するまで計算することでデータを分割する．本研究では，LRFと検出した人物の間に障害物が無いと仮定し，クラスタ数を2個に設定したk-means法を用いて，LRFから近い距離にあるクラスタを両脚部分と判定している(Fig.\ref{human_lidar_flow})．そして，ROS公式の機械学習で訓練された分類器(leg\_detector)を用いて，目標人物の脚のペアと人の中心位置を検出した．最後に，画像内の顔の有無を検出し，顔がカメラ側に向いているかを判別し，目標人物の位置・向きを推定した．

\begin{figure}[t]
\begin{center}
\includegraphics[clip, width=15cm]{figs/human_lidar_flow.eps}
\caption{System flow of Human pose stack}
\label{human_lidar_flow}
\end{center}
\end{figure}

\subsection{実験結果}
実機実験の結果をTable \ref{table:evaluation_real_world}に示す．ランダムな人の位置で1エピソードを70ステップとして9エピソードの実験を行った．実験結果より，9回のうち7回のエピソードで情報投影が成功し，2回のエピソードで70ステップ以内にエピソードが終了する結果となった．また，実験中に壁へ衝突することは無かった．情報投影成功時のUDの動きの様子をFig.\ref{real_experiment_motion}に示す．

\begin{table}[t]
	\caption{Experimental evaluation in real environment}
	\label{table:evaluation_real_world}
	\centering 
	\begin{tabular}[tbp]{|c|c|c|c|}
	    \hline
		Successfully Goal & Collision & Past 70 steps & Avg. Distance error \\\hline\hline
		7/9(78\%) & 0/9(0\%) & 2/9(22\%) & 373[mm] \\\hline
	\end{tabular}
\end{table}

\begin{figure}[t]
\begin{center}
\includegraphics[clip, width=13cm]{figs/real_experiment_motion.eps}
\caption{The motion generated by DQN algorithm in real environment}
\label{real_experiment_motion}
\end{center}
\end{figure}

\subsection{考察}
実機実験において，障害物に衝突することなく，9回のエピソードのうち7回が情報投影を成功したことから本モデルは実世界においても有用であると言える．しかし，2回にわたり情報投影できず70ステップが経過した．これは，カメラとLRFによる人の位置の誤検出によるものであると考えられる．失敗時の検出結果では，検出した人の位置が真値とは異なりシミュレーション実験で学習していない座標となっていた．よって，実世界で本モデルを活用するためには，人の位置検出精度を向上させる必要がある．

\chapter{結言}
\section{まとめ}
本研究では，情報投影ロボットが人と公共の場で共存するための行動モデルの効率化と計算量の軽量化を目的として，二光源情報投影ロボットの開発と深層強化学習を用いた行動モデルを提案した．二光源情報投影ロボットは従来の情報投影ロボットの設計コンセプトを引き継ぐと共に，より多くの人へ情報支援を可能にするためハードウェアとソフトウェアを開発した．また，両プロジェクタの映像を重ね合わせることで，より人に優しい情報支援手法を提案した．深層強化学習を用いた行動モデルに関しては，提案したニューラルネットワークモデルに目標人物に関する情報を入力すると，近接学や人の視野角に基づく快適かつ効果的に情報投影するための行動が出力可能なモデルを構築した．そして，シミュレーション実験において分散学習を用いて提案モデルを学習した後，実機を用いた動作実験により本モデルの有用性を検証した．実験の結果，エージェントを分散させて，学習することは大幅な学習時間の短縮にになることが分かった．また，実機実験においても魚眼カメラとLRFによる人物検出を実装することで，現実世界でも行動を再現することが出来た．

\section{今後の課題}
今後の課題として，複雑な環境への対応と二光源情報投影ロボットのための行動モデルの構築が挙げられる．提案したニューラルネットワークモデルでは，直立状態の人への情報投影が可能である．しかし，公共施設や都市部などの人が生活するエリアで適切に情報支援するためには，歩く人や動的な障害物への対応と全方向の環境認識が必要がある．動的な環境への対応としては，人の移動を予測して入力値に反映させることが考えられる．また，全方向の環境認識に対しては，パノラマカメラや複数の魚眼カメラを統合するとこで解決可能である．さらに，二光源情報投影ロボットが情報支援をするための行動モデルの構築に関しては，モデルベースな手法では考慮すべき条件が複雑になりすぎるため，深層強化学習で最適な行動を学習することが有効であると考えられる．

\chapter*{謝辞}
本研究を進めるにあたり，日頃から適切なご助言とご指導を与えてくださいました立命館大学情報理工学部　李周浩教授，チャン助教に心より深く感謝いたします．また，本研究に携わっていただいたすべての方に感謝いたします．最後に，学費や生活面において支えてくださった両親に感謝の意を表し，本論文の謝辞とさせていただきます．


%参考文献
% BibTeXを使用する．
% ※platexを実行（ビルド）した後にpbibtexを実行し，再度platexを実行（ビルド）しないと反映されない
% BibTexを使用しない場合は，以下の2行のコメントアウトする 
\bibliographystyle{sonota/aislab.bst} % 文献の参照スタイルの指定（aislab.bstを見に行く）
\bibliography{aislab.bib} % 文献データベース（bibファイル）の指定

%参考文献　BibTexを使用しない場合は，以下の\begin{comment}と\end{comment}の行を削除する
\begin{comment}
\begin{thebibliography}{1}
\bibitem{test1}
 athor1,  author2,  author3, ``title1'',  booktitle1, pp. 2--181--2--182, 2009.
\bibitem{test2}
 athor1,  author2, ``title2'',  booktitle2, pp. 137--142, 2010.
\bibitem{test3}
 athor1, ``title3'',  booktitle3, pp. 134--143, 2013.
\end{thebibliography}
\end{comment}

\end{document}
